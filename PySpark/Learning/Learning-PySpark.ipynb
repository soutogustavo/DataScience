{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark\n",
    "\n",
    "**Status:** *ongoing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. What is Apache Spark?\n",
    "2. Architecture\n",
    "3. Spark Ecosystem\n",
    "4. ...\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Apache Spark?\n",
    "\n",
    "[Spark](http://spark.apache.org/) is a general-purpose, distributed programming framework that was developed at the AMPLab at the University of California, Berkeley. It is open source software that provides an in-memory computation framework and it is also good for batch processing. Spark works well with real-time (or, better to say, near-real-time) data. It allows you to solve a wide variety of complex data problems whether semi-structured, structured, streaming, and/or machine learning / data sciences.\n",
    "\n",
    "Machine learning and graph algorithms are iterative. Where Spark do magic. According to its research paper, it is approximately 100 times faster than its peer, Hadoop. Data can be cached in memory. Caching intermediate data in iterative algorithms provides amazingly fast processing speed. Spark can be programmed with Java, Scala, Python, and R. Also, Spark supports multiple data sources such as Parquet, JSON, Hive, Cassandra, CSV, text files and RDBMS tables.\n",
    "\n",
    "Spark might be considered as an improved [Hadoop](https://hadoop.apache.org/). Because we can implement a [MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html) algorithm in Spark, Spark uses the benefit of HDFS; this means Spark can read data from HDFS and store data to HDFS too, and Spark handles iterative computation efficiently because data can be persisted in memory. It is good for interactive data analysis.\n",
    "\n",
    "#### Advantages of Spark\n",
    "1. **Swift Processing:** Spark reduce the number of read-write to disk.\n",
    "\n",
    "2. **Dynamic in Nature:** Spark provide 80 high-level operators, which can help to develop a parallel application. For transformations, Spark adds them to a DAG (Directed Acyclic Graph) of computation and only when the driver requests some data, does this DAG actually gets executed.\n",
    "\n",
    "3. **In-Memory Computation:** We didn’t waste our time to fetch data from disk every time, it saves time by caching data.\n",
    "\n",
    "4. **Re-Usability:** The Spark code can be reused for batch-processing, join stream against historical data.\n",
    "\n",
    "5. **Fault Tolerance:** Through RDD, it provides fault tolerance. Spark RDD are designed to handle the failure of any worker node in the cluster, which ensures that the loss of data is reduced to zero.\n",
    "\n",
    "\n",
    "<center> Logistic Regression </center>\n",
    "![logistic-regression](https://user-images.githubusercontent.com/9319823/46016970-9fb87380-c0d6-11e8-86e0-7123a95c0309.png)\n",
    "\n",
    "\n",
    "#### Disadvantages of Spark\n",
    "1. **Expensive:** In-memory capability can become a bottleneck when we want cost-efficient processing of big data as keeping data in memory is quite expensive.\n",
    "\n",
    "2. **Latency:** Apache Spark has higher latency as compared to [Apache Flink](https://flink.apache.org/).\n",
    "\n",
    "3. **Mannual Optimization:** The Spark job requires to be manually optimized and is adequate to specific datasets.\n",
    "\n",
    "4. **No File Management:** Apache Spark does not have its own file management system, thus it relies on some other platform like Hadoop.\n",
    "\n",
    "5. **Problem With Small Files:** If we use Spark with Hadoop, we come across a problem of a small file. HDFS provides a limited number of large files rather than a large number of small files. Another place where Spark legs behind is we store the data gzipped in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark_architecture](https://user-images.githubusercontent.com/9319823/45994904-09645d80-c096-11e8-87e4-2b53f058ba99.png)\n",
    "\n",
    "The main components of the Spark architecture are the driver and executors. For each PySpark application, there will be one driver program and one or more executors running on the cluster slave machine. Therefore, Spark follows a master/slave architecture.\n",
    "\n",
    "- **Driver process / Master (Master Daemon):** The driver is the process that coordinates with many executors running on various slave machines. Spark follows a master/slave architecture.\n",
    "\n",
    "    - The ***SparkContext*** object is created by the driver, and it is the main entry point to a (Py)Spark application.\n",
    "    - Spark Driver also contains various components such as *DAGScheduler*, *TaskScheduler*, *BackendScheduler* and *BlockManager* which is responsible for the translation of spark user code into actual spark jobs executed on the cluster. \n",
    "\n",
    "\n",
    "- **Executors / Slaves (Worker Daemon):** Executors are slave processes. An executor runs tasks. It also has the capability to cache data in memory.\n",
    "\n",
    "    - Executor is a distributed agent responsible for the execution of tasks. Every spark applications has its own executor process.\n",
    "    - They usually run for the entire lifetime of a Spark application and this phenomenon is known as **“Static Allocation of Executors”**.\n",
    "    - However, users can also opt for dynamic allocations of executors wherein they can add or remove spark executors dynamically to match with the overall workload.\n",
    "    - Executor performs all the data processing.\n",
    "    - Reads from and Writes data to external sources.\n",
    "    - Executor stores the computation results data in-memory, cache or on hard disk drives.\n",
    "    - Interacts with the storage systems.\n",
    "    \n",
    "    \n",
    "- **Cluster Manager:** An external service responsible for acquiring resources on the spark cluster and allocating them to a spark job.\n",
    "    - Hadoop YARN, Apache Mesos are examples of cluster manager.\n",
    "    - Locally - simple standalone spark cluster manager.\n",
    "    - Choosing a cluster manager for any spark application depends on the goals of the application because all cluster managers provide different set of scheduling capabilities.\n",
    "\n",
    "The driver breaks our application into small tasks; a task is the smallest unit of your application. Tasks are run on different executors in parallel. The driver is also responsible for scheduling tasks to different executors. Also, The **cluster manager** manages cluster resources. The driver talks to the cluster manager to negotiate resources. The cluster manager also schedules tasks on behalf of the driver on various slave executor processes.\n",
    "\n",
    "Spark is dispatched with Standalone Cluster Manager. However, it can also be configured on [YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) and [Apache Mesos](http://mesos.apache.org/). Spark can be also started in local mode too (i.e. on a single machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Spark Ecosystem\n",
    "\n",
    "<br>\n",
    "\n",
    "![spark-stack](https://user-images.githubusercontent.com/9319823/45998657-ca3d0900-c0a3-11e8-8bb8-32672e87d119.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Spark ecosytem has six components: [Spark core](https://spark.apache.org/docs/1.6.0/index.html), [SQL and DataFrames](http://spark.apache.org/sql/), [MLlib](http://spark.apache.org/mllib/) for machine learning, [GraphX](http://spark.apache.org/graphx/), [Spark Streaming](http://spark.apache.org/streaming/), and [SparkR](https://spark.apache.org/docs/1.6.0/sparkr.html). You can combine these libraries seamlessly in the same application.\n",
    "\n",
    "#### **Spark Core:** \n",
    "\n",
    "All the functionalities being provided by Apache Spark are built on the top of Spark Core. It delivers speed by providing in-memory computation capability. Thus Spark Core is the foundation of parallel and distributed processing of huge dataset.\n",
    "\n",
    "Spark Core is embedded with a special collection called **RDD** (resilient distributed dataset). RDD is among the abstractions of Spark. **Spark RDD handles partitioning data across all the nodes in a cluster**. It holds them in the memory pool of the cluster as a single unit. There are two operations performed on RDDs: Transformation and Action.\n",
    "   - **Transformation:** It is a function that produces new RDD from the existing RDDs.\n",
    "   - **Action:** In Transformation, RDDs are created from each other. But when we want to work with the actual dataset, then, at that point we use Action.\n",
    "\n",
    "\n",
    "**Key features of Spark Core are:** essential I/O functionalities, task dispatching, fault recovery, and significant in programming and observing the role of the Spark cluster.\n",
    "\n",
    "#### **SparkSQL:** \n",
    "\n",
    "SparkSQL library is a wrapper over the PySpark core that applies SQL-like analysis on a huge amount of structured or semistructured data. We can also use SQL queries with PySparkSQL. We can connect it to Apache Hive, and HiveQL can be applied too. PySparkSQL introduced the DataFrame, which is a tabular representation of structured data that is like a table in a relational database management system.\n",
    "\n",
    "It is a distributed framework for structured data processing. Using Spark SQL, **Spark gets more information about the structure of data and the computation**. With this information, Spark can perform extra optimization. It uses same execution engine while computing an output. It **does not depend on API/ language to express the computation**.\n",
    "\n",
    "It also enables powerful, interactive, analytical application across both streaming and historical data. Spark SQL is Spark module for structured data processing. Thus, it acts as a distributed SQL query engine.\n",
    "\n",
    "**Key features of SparkSQL include:** cost based optimizer, mid query fault-tolerance, full compatibility with existing Hive data, etc.\n",
    "\n",
    "\n",
    "#### **SparkML:** \n",
    "\n",
    "MLlib is a wrapper over the PySpark core that deals with machine-learning algorithms. The machine-learning API provided by the MLlib library is easy to use. MLlib supports many machine-learning algorithms for classification, clustering, text analysis, and more. Also, some lower level machine learning primitives like generic gradient descent optimization algorithm are also present in MLlib.\n",
    "\n",
    "In Spark Version 2.0, the DataFrame-based API is the primary Machine Learning API for Spark. So, from now MLlib will not add any new feature to the RDD based API. The reason behind this is that it is more user-friendly than RDD. Some of the benefits of using DataFrames are it includes Spark Data sources, SQL DataFrame queries Tungsten and Catalyst optimizations, and uniform APIs across languages. MLlib also uses the **linear algebra package Breeze**. Breeze is a collection of libraries for numerical computing and machine learning.\n",
    "\n",
    "\n",
    "#### **GraphX:** \n",
    "\n",
    "GraphX is Apache Spark's API for graphs and graph-parallel computation. It unifies ETL, exploratory analysis, and iterative graph computation within a single system. You can view the same data as both graphs and collections, transform and join graphs with RDDs efficiently, and write custom iterative graph algorithms using the Pregel API.\n",
    "\n",
    "**Clustering, classification, traversal, searching, and pathfinding** is also possible in graphs. Furthermore, GraphX extends Spark RDD by bringing in light a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. GraphX also optimizes the way in which we can represent vertex and edges when they are primitive data types. To support graph computation it supports fundamental operators (e.g., subgraph, join Vertices, and aggregate Messages) as well as an optimized variant of the Pregel API.\n",
    "\n",
    "\n",
    "#### **Spark Streaming:** \n",
    "\n",
    "Spark Streaming brings Apache Spark's language-integrated API to stream processing, letting you write streaming jobs the same way you write batch jobs. It supports Java, Scala and Python. It is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n",
    "\n",
    "Spark can access data from sources like Kafka, Flume, Kinesis or TCP socket. The data so received is given to file system, databases and live dashboards. Spark uses Micro-batching for real-time streaming. **Micro-batching** is a technique that allows a process or task to treat a stream as a sequence of small batches of data. Hence Spark Streaming, groups the live data into small batches. \n",
    "\n",
    "Spark Streaming works in three phases: **(1) gathering**, **(2) processing**, and **(3) data storage**.\n",
    "  1. It provides two categories of built-in streaming sources: \n",
    "      - **Basic sources:** file systems and socket connections; and \n",
    "      - **Advanced sources:** sources like Kafka, Flume, Kinesis, etc.;\n",
    "  2. The gathered data is processed using complex algorithms expressed with a high-level function.\n",
    "  3. The Processed data is pushed out to file systems, databases, and live dashboards.\n",
    "\n",
    "**DStream** in Spark signifies continuous stream of data. We can form DStream in two ways either from sources such as Kafka, Flume, and Kinesis or by high-level operations on other DStreams. Thus, DStream is internally a sequence of RDDs.\n",
    "\n",
    "![streaming-arch](https://user-images.githubusercontent.com/9319823/45999822-150c5000-c0a7-11e8-8a8a-f88b2c5b1c88.png)\n",
    "\n",
    "\n",
    "#### **SparkR:** \n",
    "\n",
    "The key component of SparkR is SparkR DataFrame. DataFrames are a fundamental data structure for data processing in R. The concept of DataFrames extends to other languages with libraries like Pandas etc.\n",
    "\n",
    "**R also provides software facilities for data manipulation, calculation, and graphical display**. Hence, the main idea behind SparkR was to explore different techniques to integrate the usability of R with the scalability of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Kumar, R., 2018. ***PySpark Recipes***. Apress.\n",
    "2. Tomasz, D., 2017. ***Learning PySpark***. Packt Publishing.\n",
    "3. Spark.apache.org. (2018). ***Apache Spark™ - Unified Analytics Engine for Big Data***. [online] Available at: http://spark.apache.org/ [Accessed 25 Sep. 2018].\n",
    "4. Spark.apache.org. (2018). ***GraphX | Apache Spark***. [online] Available at: http://spark.apache.org/graphx/ [Accessed 25 Sep. 2018].\n",
    "5. Spark.apache.org. (2018). ***Spark Streaming - Spark 2.3.1 Documentation***. [online] Available at: https://spark.apache.org/docs/latest/streaming-programming-guide.html [Accessed 25 Sep. 2018].\n",
    "6. DeZyre. (2018). **Apache Spark Architecture Explained in Detail**. [online] Available at: https://www.dezyre.com/article/apache-spark-architecture-explained-in-detail/338 [Accessed 26 Sep. 2018]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
