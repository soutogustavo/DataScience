{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark\n",
    "\n",
    "**Status:** *ongoing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. What is Apache Spark?\n",
    "2. Architecture\n",
    "3. Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Apache Spark?\n",
    "\n",
    "[Spark](http://spark.apache.org/) is a general-purpose, distributed programming framework that was developed at the AMPLab at the University of California, Berkeley. It is open source software that provides an in-memory computation framework and it is also good for batch processing. Spark works well with real-time (or, better to say, near-real-time) data.\n",
    "\n",
    "Machine learning and graph algorithms are iterative. Where Spark do magic. According to its research paper, it is approximately 100 times faster than its peer, Hadoop. Data can be cached in memory. Caching intermediate data in iterative algorithms provides amazingly fast processing speed. Spark can be programmed with Java, Scala, Python, and R.\n",
    "\n",
    "Spark might be considered as an improved [Hadoop](https://hadoop.apache.org/). Because we can implement a [MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html) algorithm in Spark, Spark uses the benefit of HDFS; this means Spark can read data from HDFS and store data to HDFS too, and Spark handles iterative computation efficiently because data can be persisted in memory. It is good for interactive data analysis.\n",
    "\n",
    "<br>\n",
    "\n",
    "![spark-stack](https://user-images.githubusercontent.com/9319823/45998657-ca3d0900-c0a3-11e8-8bb8-32672e87d119.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Spark powers a stack of libraries including [SQL and DataFrames](http://spark.apache.org/sql/), [MLlib](http://spark.apache.org/mllib/) for machine learning, [GraphX](http://spark.apache.org/graphx/), and [Spark Streaming](http://spark.apache.org/streaming/). You can combine these libraries seamlessly in the same application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark_architecture](https://user-images.githubusercontent.com/9319823/45994904-09645d80-c096-11e8-87e4-2b53f058ba99.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**Note:** Adjust references\n",
    "\n",
    "1. Raju Kumar Mishra (auth.)- PySpark Recipes_ A Problem-Solution Approach with PySpark2-Apress (2018)\n",
    "2. http://spark.apache.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
